{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Evaluation\n",
    "\n",
    "This notebook shows how to produce some tables and plots for analysing results. This is meant as a starting point, because eah game will require a custom evaluation depending on its metrics and experiments.\n",
    "\n",
    "This notebook uses the output files of evaluation/bencheval.py. Run that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import evaluation.evalutils as utils\n",
    "from evaluation.makingtables import build_dispersion_table\n",
    "import clemgame.metrics as clemmetrics"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose path for the .csv with the raw scores. This is created after running ```bencheval.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "RAW_PATH = Path('../results/raw.csv')\n",
    "raw_df = pd.read_csv(RAW_PATH, index_col=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the contents of ```data```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "raw_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to evaluate a single game and the results directory contained more games, we can filter the game column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "game_df = raw_df[raw_df.game == 'privateshared']\n",
    "game_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dispersion metrics over all episodes. Note that we use groupby, which will ignore NaN values, according to the documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "stats_df = build_dispersion_table(['game', 'model', 'metric'], game_df) # could also use raw_df for results for all games\n",
    "stats_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break results down by experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "detailed_stats_df = build_dispersion_table(['game', 'model', 'experiment', 'metric'], game_df) # could also use raw_df for results for all games\n",
    "detailed_stats_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the table, call ```to_csv```, ```to_html``` and/or ```to_latex``` (there are also other formats and customization options, check the pandas docs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "#stats_df.to_csv('../results/stats.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots can of course be created with ```matplotlib``` directly, but ```seaborn``` makes it very easy to work with pandas dataframes. Let's create a barplot with an overview by model and experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# choose a metric\n",
    "METRIC = 'Main Score'\n",
    "metric_df = game_df[game_df.metric == METRIC].sort_values('experiment')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# change the figure size according to your needs\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# choose what goes in each axis and legend according to your needs\n",
    "# you can also e.g. swap y and hue, to get games in the legend and models in the axis\n",
    "g = sns.barplot(metric_df,\n",
    "                x='experiment',\n",
    "                y='value',\n",
    "                hue='model', \n",
    "                ax=ax)\n",
    "\n",
    "# customize plot details according to your needs\n",
    "sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.ylim(0, 100)\n",
    "plt.ylabel(METRIC)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark plots\n",
    "\n",
    "Reproducing Figure 10 in the paper. As more models or games are added, sizes must be adjusted accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# read the clem scores\n",
    "RESULTS_PATH = Path('../results/results.csv')\n",
    "clem_df = pd.read_csv(RESULTS_PATH, index_col=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizontal bar plot with player, aborted, lost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "STACK_COLORS = ['darkolivegreen', 'indianred', 'gray']\n",
    "\n",
    "df_aux = raw_df[raw_df.metric.isin(utils.GAMEPLAY_METRICS)]\n",
    "\n",
    "df_aux = (df_aux.pivot(index=['game', 'model', 'experiment', 'episode'],\n",
    "                       columns='metric',\n",
    "                       values='value')\n",
    "                .reset_index()\n",
    "                .drop(columns=['game', 'experiment', 'episode'])\n",
    "                .groupby('model')\n",
    "                .sum()\n",
    "                .sort_values(axis=1, by='metric', ascending=False))\n",
    "percs = 100 * df_aux.div(df_aux.sum(axis=1), axis=0)\n",
    "order = clem_df.sort_values(by='-, clemscore').index\n",
    "percs = percs.reindex(order)\n",
    "\n",
    "percs.plot(kind='barh',\n",
    "           stacked=True,\n",
    "           figsize=(5, 5),\n",
    "           colormap=ListedColormap(STACK_COLORS))\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncols=3)\n",
    "plt.xlabel('% of all episodes')\n",
    "plt.xlim(-1, 101)\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot with the sorted clemscores (they were not sorted in the paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "clem_aux = clem_df.sort_values(by='-, clemscore').reset_index(names='model')\n",
    "# below is a hack to make the current names fit the plot\n",
    "# ideally a dictionary with short names should be used as in the original implementation for the paper\n",
    "# warning, if models have matching names up the limit, results will be wrongly merged!\n",
    "clem_aux['model'] = clem_aux['model'].apply(lambda x: f'{x[:10]}\\n{x[10:20]}\\n{x[20:30]}\\n{x[30:]}')\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "sns.barplot(clem_aux, x='model', y='-, clemscore', color='slategray')\n",
    "plt.ylim(-5, 105)\n",
    "plt.ylabel('clemscore')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polygons. This can get complicated if more games are added. In particular, COLUMN_ORDER must be adjusted. For more models, the number of subplots and figsize must be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "def ccw_sort(p):\n",
    "    \"\"\"Put the nodes in clockwise order.\"\"\"\n",
    "    # from https://stackoverflow.com/a/44143444 by user ImportanceOfBeingEarnest\n",
    "    p = np.array(p)\n",
    "    mean = np.mean(p, axis=0)\n",
    "    d = p - mean\n",
    "    s = np.arctan2(d[:, 0], d[:, 1])\n",
    "    return p[np.argsort(s), :]\n",
    "\n",
    "\n",
    "fig, ax_list = plt.subplots(3, 4, figsize=(9, 6), sharey=True, sharex=True)\n",
    "axs = ax_list.flatten()\n",
    "\n",
    "for n, (model, model_df) in enumerate(raw_df.groupby('model')):\n",
    "    rows = model_df.metric.isin(utils.MAIN_METRICS)\n",
    "    df_aux = model_df[rows]\n",
    "    df_aux = (df_aux.pivot(index=['game', 'experiment', 'episode'],\n",
    "                            columns='metric',\n",
    "                            values='value')\n",
    "                    .reset_index())\n",
    "    df_aux = df_aux.drop(['episode'], axis=1)\n",
    "\n",
    "    # create the x and y coordinates for each game\n",
    "    dots = []\n",
    "    for game, game_df in df_aux.groupby('game'):\n",
    "        overall_means = (game_df.mean(numeric_only=True)\n",
    "                                .fillna(0))\n",
    "        # replace missing score by 0 when all aborted\n",
    "        played = overall_means[clemmetrics.METRIC_PLAYED] * 100\n",
    "        score = overall_means[clemmetrics.BENCH_SCORE]\n",
    "        dots.append((game, played, score))\n",
    "    labels, played, scores = zip(*dots)\n",
    "    # put them in a good order for the polygon\n",
    "    edges = ccw_sort(list(zip(played, scores)))\n",
    "\n",
    "    # create the polygon and draw it\n",
    "    polygon = Polygon(edges, facecolor='lightgray')\n",
    "    axs[n].add_patch(polygon)\n",
    "\n",
    "    legend = True if n == 10 else False\n",
    "    g = sns.scatterplot(x=played,\n",
    "                        y=scores,\n",
    "                        hue=labels,\n",
    "                        style=labels,\n",
    "                        hue_order=utils.COLUMN_ORDER[1:],\n",
    "                        style_order=utils.COLUMN_ORDER[1:],\n",
    "                        s=80,\n",
    "                        ax=axs[n],\n",
    "                        legend=legend)\n",
    "    axs[n].set_xlim(-5, 105)\n",
    "    axs[n].set_ylim(-5, 105)\n",
    "    axs[n].set_ylabel('avr. quality')\n",
    "    axs[n].set_xlabel('% played')\n",
    "    axs[n].set_title(model, fontsize=8)\n",
    "\n",
    "fig.legend(loc='lower right', bbox_to_anchor=(0.98, 0.08))\n",
    "axs[10].legend().set_visible(False)\n",
    "fig.delaxes(axs[11])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clembench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
